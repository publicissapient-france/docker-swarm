---
layout: main
css: ../stylesheets/stylesheet.css
js: ../javascripts/main.js
---

<p>Pour ce hands'on, vous aurez à votre disposition 3 instances Amazon AWS.
Il sera impératif de les utiliser pour permettre le bon fonctionnement du cluster Swarm !</p>

<p>Toutes les machines utilisent la même clef ssh, et ont le même login <strong><em>docker</em></strong>.</p>

<p>Pour valider le fonctionnement de vos instances, connectez-vous avec la commande suivante (le mot de passe vous sera donné en séance) : </p>

{% highlight bash %}
$ ssh -l docker swarm-{{page.groupId}}.aws.xebiatechevent.info
{% endhighlight %}

<h2>Initialiser un cluster Swarm</h2>

<h3>Préparation de l'infrastructure</h3>

<p>Swarm se compose d'un manager, qui joue le rôle de serveur connaissant l'ensemble des noeuds, et d'agents,
lancés sur les machines hébergeant les Docker Engine. L'ensemble du système repose sur un système de
découverte de services externe, qui peut être fourni par Consul, Etcd ou Zookeeper.</p>

<p>Vous avez 3 machines à votre disposition. La première jouera le rôle de Master sur laquelle nous allons
installer le système de découverte de services externe (dans notre cas Consul), ainsi que le Swarm Manager.</p>

<p>Les deux autres machines joueront le rôle de slave sur lesquelles nous allons installer un agent Swarm.</p>

<h3>Installation de Consul</h3>

<p>Sur la première machine, lancez une instance de Consul à l'aide de la commande suivante :</p>

{% highlight bash %}
$ docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap
{% endhighlight %}

<p>Ceci va démarrer un serveur Consul standalone avec le port 8500 d'exposé, 
suffisant pour l'utilisation de Docker Swarm.</p>

<h3>Initialisation du cluster</h3>

<p>Maintenant que nous avons une instance de Consul de disponible, nous pouvons 
initialiser le cluster !</p>

<p>Lancez la commande suivante toujours sur la première machine :</p>

{% highlight bash %}
$ docker run -d -p 4000:4000 swarm manage -H :4000 --advertise <master_ip>:4000 consul://<consul_ip>:8500
{% endhighlight %}

<p>Cette commande démarre un manager écoutant sur le port 4000, et s'annonçant dans le Consul donné.
Remplacez master_ip par l'adresse IP du Swarm Manager (il doit s'annoncer lui-même 
auprès du système de découverte de service) et consul_ip par l'IP du serveur hébergeant 
le consul déployé précédemment.</p>

<h2>Intégrer le cluster Swarm</h2>

<p>Avec notre manager Swarm disponible, nous allons pouvoir intégrer les noeuds Docker Engine dans le cluster !</p>

<h3>Rejoindre le cluster</h3>

<p>Avant de lancer l'agent Swarm, assurez-vous que votre démon Docker est bien configuré pour
rejoindre le cluster.</p>

<p>Pour cela, éditez le fichier <strong><em>/etc/default/docker</em></strong>.</p>

{% highlight bash %}
$ sudo vi /etc/default/docker
{% endhighlight %}

<p>Remplacez <strong><em>127.0.0.1</em></strong> par <strong><em>0.0.0.0</em></strong>.</p>

{% highlight bash %}
DOCKER_OPTS="-H tcp://0.0.0.0:2375"
{% endhighlight %}

<p>Redémarer le démon Docker afin que la modification soit prise en compte</p>

{% highlight bash %}
$ sudo systemctl restart docker
{% endhighlight %}

<p>Sur chacun des noeuds, lancez la commande suivante pour rejoindre le cluster :</p>

{% highlight bash %}
$ docker run -d swarm join --advertise=<node_ip>:2375 consul://<consul_ip>:8500
{% endhighlight %}

<p>Remplacez node_ip par l'IP locale du noeud et consul_ip par l'IP du serveur Consul.
Si la commande s'est bien passée, le noeud a normalement rejoint le cluster, il n'y a plus qu'à vérifier.</p>

<h3>Explorer le cluster</h3>

<p>Swarm exposant la même API que celle des Engine Docker, il est très simple
de se connecter à un manager. Sur votre machine, lancez la commande suivante :</p>

{% highlight bash %}
$ docker -H <manager_ip>:4000 info
Containers: 6
 Running: 6
 Paused: 0
 Stopped: 0
Images: 12
Server Version: swarm/1.1.3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 2
 52.28.227.0: 52.28.227.0:2375
  └ Status: Healthy
  └ Containers: 3
  └ Reserved CPUs: 0 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T15:28:03Z
 52.29.87.59: 52.29.87.59:2375
  └ Status: Healthy
  └ Containers: 3
  └ Reserved CPUs: 0 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T15:27:38Z
Plugins: 
 Volume: 
 Network: 
Kernel Version: 4.2.0-19-generic
Operating System: linux
Architecture: amd64
CPUs: 4
Total Memory: 15.34 GiB
Name: dad66ef64043
{% endhighlight %}

<p>Vous devriez avoir une sortie ressemblant à cela, avec un nombre variable de
noeuds en fonction du nombre de personnes ayant rejoint le cluster. Si l'IP de
votre machine est présente, vous avez bien rejoint le cluster !</p>

<p>Une méthode plus rapide pour lister tous les noeuds du cluster est d'utiliser
le client swarm comme ceci :</p>

{% highlight bash %}
$ docker run --rm swarm list consul://<consul_ip>:8500
time="2016-03-06T13:34:21Z" level=info msg="Initializing discovery without TLS"
10.10.0.113:2375
{% endhighlight %}

<h2>Démarrer des conteneurs sur Swarm</h2>

<h3>Configurer son client Docker</h3>

<p>Afin de pouvoir facilement contrôler le cluster Swarm, vous pouvez faire pointer 
votre client Docker sur le Swarm Manager en utilisant la variable d'environnement
DOCKER_HOST</p>

{% highlight bash %}
$ export DOCKER_HOST=tcp://<manager_ip>:4000
{% endhighlight %}

<p>Il est également possible d'indiquer le host correspondant au Swarm Manager en option
de chaque commande Docker grâce à l'option -H</p>

{% highlight bash %}
$ docker -H <manager_ip>:4000 <docker_cmd>
{% endhighlight %}

<p>Vérifier votre client Docker</p>

{% highlight bash %}
$ docker version
Client:
 Version:      1.10.2
 API version:  1.22
 Go version:   go1.5.3
 Git commit:   c3959b1
 Built:        Mon Feb 22 21:40:35 2016
 OS/Arch:      linux/amd64

Server:
 Version:      swarm/1.1.3
 API version:  1.22
 Go version:   go1.5.3
 Git commit:   7e9c6bd
 Built:        Wed Mar  2 00:15:12 UTC 2016
 OS/Arch:      linux/amd64
{% endhighlight %}

<h3>Commandes de base</h3>

<p>Lancer des conteneurs sur un cluster Swarm est aussi simple que sur un Docker Engine local,
seuls quelques paramètres supplémentaires apparaissent ainsi que de nouvelles informations
sur la sortie du client Docker.</p>

<p>Lancer une instance de Nginx</p>

{% highlight bash %}
$ docker run -d nginx
cd4933ec58cbe457508a498926945a32bf983a688b878d3771257830ddf7be73
{% endhighlight %}

<p>Lister les conteneurs lancés au sein du cluster</p>

{% highlight bash %}
$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
cd4933ec58cb        nginx               "nginx -g 'daemon off"   14 seconds ago      Up 14 seconds       80/tcp, 443/tcp     52.28.211.86/prickly_mayer
{% endhighlight %}

<p>On voit ici que les noms des conteneurs dockers sont agrémentés d'un préfixe supplémentaire,
donnant le nom du noeud sur lequel le conteneur a été démarré.</p>

<p>Il reste possible d'administrer directement les noeuds unitairement, en faisant pointer
le client Docker vers le noeud à administrer.</p>

<p>Vous pouvez utiliser toutes les mêmes commandes Docker au sein d'un cluster Swarm, une fois
que le client Docker soit bien connecté au Swarm Manager.</p>

<p>Par exemple, vous pouvez vérifier les logs d'un conteneur en utilisant la commande logs</p>

{% highlight bash %}
$ docker logs prickly_mayer
{% endhighlight %}

<p>La commande inspect fonctionne aussi de la même manière. Il n'est pas nécessaire d'indiquer 
le noeud dans le nom lorsque l'on utilise les commandes Docker.</p>

{% highlight bash %}
$ docker inspect prickly_mayer
[
    {
        "Id": "cd4933ec58cbe457508a498926945a32bf983a688b878d3771257830ddf7be73",
        "Created": "2016-03-06T13:40:29.322115236Z",
        "Path": "nginx",
        "Args": [
            "-g",
            "daemon off;"
        ],
{% endhighlight %}

<p>La commande images donne une vue combinées de l'ensemble des images présentes au sein du
cluster.</p>

{% highlight bash %}
$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
fedora              latest              ddd5c9c1d0f2        45 hours ago        204.7 MB
centos              latest              d0e7f81ca65c        46 hours ago        196.6 MB
ubuntu              latest              07c86167cdc4        2 days ago          188 MB
redis               latest              4f5f397d4b7c        3 days ago          177.6 MB
nginx               latest              fd19524415dc        4 days ago          134.6 MB
swarm               latest              291cbe419fe6        4 days ago          18.11 MB
debian              latest              f50f9524513f        4 days ago          125.1 MB
{% endhighlight %}

<h3>Les stratégies d'orchestration</h3>

<p>Docker Swarm a la capacité d'orchestrer les conteneurs au sein du cluster en s'appuyant
sur différentes stratégies. Lorsque vous lancer un conteneur, Swarm l'exécutera sur le noeud
ayant le rang le plus élevé. Le calcul du rang dépend de la stratégie utilisée :
Spread (par défaut), Binpack ou Random.</p>

<p>La stratégie Spread classe les noeuds selon le nombre de conteneurs lancés. Swarm lancera un nouveau
conteneur sur le noeud qui possède le moins de conteneurs en cours d'execution.</p>

<p>Lancer à nouveau plusieurs instances de Nginx.</p>

{% highlight bash %}
$ docker run -d nginx
{% endhighlight %}

<p>Vérifier la répartition des conteneurs à l'aide de la commande docker ps.</p>

{% highlight bash %}
$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
b1420393c045        nginx               "nginx -g 'daemon off"   4 seconds ago       Up 4 seconds        80/tcp, 443/tcp     52.28.227.0/modest_mestorf
42b3a887aa9b        nginx               "nginx -g 'daemon off"   5 seconds ago       Up 5 seconds        80/tcp, 443/tcp     52.29.87.59/adoring_lalande
1d321f3b9127        nginx               "nginx -g 'daemon off"   6 seconds ago       Up 6 seconds        80/tcp, 443/tcp     52.28.227.0/prickly_panini
bb1c096b3be4        nginx               "nginx -g 'daemon off"   7 seconds ago       Up 6 seconds        80/tcp, 443/tcp     52.29.87.59/pedantic_shirley
930656e67e77        nginx               "nginx -g 'daemon off"   7 seconds ago       Up 7 seconds        80/tcp, 443/tcp     52.29.87.59/ecstatic_poitras
2394bbe3ebce        nginx               "nginx -g 'daemon off"   8 seconds ago       Up 8 seconds        80/tcp, 443/tcp     52.28.227.0/stupefied_stonebraker
{% endhighlight %}

<p>Vous pouvez également vérifier le nombre de conteneurs lancés sur chaque noeud
avec la commande docker info.</p>

{% highlight bash %}
$ docker info
Containers: 8
 Running: 8
 Paused: 0
 Stopped: 0
Images: 14
Server Version: swarm/1.1.3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 2
 52.28.227.0: 52.28.227.0:2375
  └ Status: Healthy
  └ Containers: 4
  └ Reserved CPUs: 0 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T16:44:56Z
 52.29.87.59: 52.29.87.59:2375
  └ Status: Healthy
  └ Containers: 4
  └ Reserved CPUs: 0 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T16:44:59Z
Plugins: 
 Volume: 
 Network: 
Kernel Version: 4.2.0-19-generic
Operating System: linux
Architecture: amd64
CPUs: 4
Total Memory: 15.34 GiB
Name: dad66ef64043
{% endhighlight %}

<p>Pour arrêter et supprimer rapidement l'ensemble des conteneurs, vous pouvez utiliser
la commande suivante :</p>

{% highlight bash %}
$ docker rm -fv $(docker ps -q)
{% endhighlight %}

<p>Lorsque l'on utilise la stratégie Binpack, Swarm va tenter de lancer un maximum de
conteneurs au sein d'un même noeud avant d'en utiliser un autre. Pour cela il va
s'appuyer sur les resources CPU et mémoire de chaque noeud. Il est donc nécessaire
de spécifier les besoins en terme de CPU et de mémoire lorsque l'on exécute un conteneur
au sein d'un cluster utilisant la stratégie Binpack. Si aucune restriction n'est précisé
au lancement d'un conteneur, celui-ci sera systématiquement lancé sur le premier noeud
du cluster.</p>

<p>Configurez votre cluster Swarm pour utiliser la stratégie Binpack en commencant
par récupérer l'identifiant de votre Swarm Manager sur la machine Master.</p>

{% highlight bash %}
$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                                                            NAMES
dad66ef64043        swarm               "/swarm manage -H :40"   2 hours ago         Up 2 hours          2375/tcp, 0.0.0.0:4000->4000/tcp                                                 mad_turing
{% endhighlight %}

<p>Arrêtez le Swarm Manager.</p>

{% highlight bash %}
$ docker rm -fv dad66ef64043 
{% endhighlight %}

<p>Relancer une nouvelle instance de Swarm Manager en indiquant la stratégie Binpack.</p>

{% highlight bash %}
$ docker run -d -p 4000:4000 swarm manage --strategy binpack -H :4000 --advertise <master_ip>:4000 consul://<consul_ip>:8500
{% endhighlight %}

<p>Vérifiez que la nouvelle stratégie a bien été configuré en lançant la commande
docker info sur un des noeud du cluster.</p>

{% highlight bash %}
$ docker info
Containers: 8
 Running: 8
 Paused: 0
 Stopped: 0
Images: 14
Server Version: swarm/1.1.3
Role: primary
Strategy: binpack
Filters: health, port, dependency, affinity, constraint
Nodes: 2
 52.28.227.0: 52.28.227.0:2375
  └ Status: Healthy
  └ Containers: 4
  └ Reserved CPUs: 0 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: disk=ssd, executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T17:01:50Z
 52.29.87.59: 52.29.87.59:2375
  └ Status: Healthy
  └ Containers: 4
  └ Reserved CPUs: 0 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T17:01:50Z
Plugins: 
 Volume: 
 Network: 
Kernel Version: 4.2.0-19-generic
Operating System: linux
Architecture: amd64
CPUs: 4
Total Memory: 15.34 GiB
Name: 7563fa19c208
{% endhighlight %}

<p>Lancez plusieurs instances de Nginx en réservant 200 mb de mémoire pour chaque instance, 
en utilisant l'option -m.</p>

{% highlight bash %}
$ docker run -d -m 2000MB nginx
{% endhighlight %}

<p>Vérifiez que les premières instances de Nginx sont bien lancées sur la premier noeud du cluster,
puis une fois qu'il n'y a plus suffisament de resource disponible, les instances suivantes sont
lancés sur le second noeud.</p>

{% highlight bash %}
$ docker info
Containers: 6
 Running: 6
 Paused: 0
 Stopped: 0
Images: 14
Server Version: swarm/1.1.3
Role: primary
Strategy: binpack
Filters: health, port, dependency, affinity, constraint
Nodes: 2
 52.28.227.0: 52.28.227.0:2375
  └ Status: Healthy
  └ Containers: 2
  └ Reserved CPUs: 0 / 2
  └ Reserved Memory: 1.953 GiB / 7.669 GiB
  └ Labels: disk=ssd, executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T17:08:52Z
 52.29.87.59: 52.29.87.59:2375
  └ Status: Healthy
  └ Containers: 4
  └ Reserved CPUs: 0 / 2
  └ Reserved Memory: 5.859 GiB / 7.669 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T17:08:47Z
Plugins: 
 Volume: 
 Network: 
Kernel Version: 4.2.0-19-generic
Operating System: linux
Architecture: amd64
CPUs: 4
Total Memory: 15.34 GiB
Name: 7563fa19c208
{% endhighlight %}

<p>N'oubliez pas de nettoyer votre environnement en utilisant la commande suivante :</p>

{% highlight bash %}
$ docker rm -fv $(docker ps -q)
{% endhighlight %}

<p>La stratégie Random n'utilise aucun algorithme de classement et exécute simplement
chaque conteneur sur un noeud du cluster choisi aléatoirement. Cette stratégie n'a
d'intérêt que pour des tests ou du debug.</p>

<h3>Gestion des resources</h3>

<p>Lancez plusieurs instances de Redis en réservant 2 CPU pour chaque instance, 
en utilisant l'option --cpu-shares.</p>

{% highlight bash %}
$ docker run -d --cpu-shares 2 redis
{% endhighlight %}

<p>Vous ne devriez pas pouvoir lancer plus de 2 instances de Redis et avoir le message
d'erreur suivant une fois que l'ensemble des resources soit utilisé :</p>

{% highlight bash %}
docker: Error response from daemon: no resources available to schedule container.
See 'docker run --help'.
{% endhighlight %}

<p>Vérifier l'utilisation des resources au sein du cluster avec la commande docker info</p>

{% highlight bash %}
$ docker info
Containers: 8
 Running: 8
 Paused: 0
 Stopped: 0
Images: 14
Server Version: swarm/1.1.3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 2
 52.28.227.0: 52.28.227.0:2375
  └ Status: Healthy
  └ Containers: 4
  └ Reserved CPUs: 2 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T15:43:03Z
 52.29.87.59: 52.29.87.59:2375
  └ Status: Healthy
  └ Containers: 4
  └ Reserved CPUs: 2 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T15:42:47Z
Plugins: 
 Volume: 
 Network: 
Kernel Version: 4.2.0-19-generic
Operating System: linux
Architecture: amd64
CPUs: 4
Total Memory: 15.34 GiB
Name: dad66ef64043
{% endhighlight %}

<p>Vous pouvez également lancer plusieurs instances de Redis en mappant le port 6379 sur le host.</p>

{% highlight bash %}
$ docker run -d -p 6379:6379 redis
{% endhighlight %}

<p>Vous ne devriez pas pouvoir lancer plus d'instance que vous avez de noeud au sein de votre cluster.
Une instance par noeud. Le message suivant devrait ensuite s'afficher :</p>

{% highlight bash %}
docker: Error response from daemon: unable to find a node with port 6379 available.
See 'docker run --help'.
{% endhighlight %}

<p>Vous pouvez vérifier l'emplacement des conteneurs Redis au sein de votre cluster en utilisant label
commande docker ps.</p>

{% highlight bash %}
$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                         NAMES
1c561511b2b1        redis               "/entrypoint.sh redis"   3 seconds ago       Up 3 seconds        52.29.87.59:6379->6379/tcp    52.29.87.59/insane_stonebraker
6aa745068037        redis               "/entrypoint.sh redis"   4 seconds ago       Up 4 seconds        52.28.227.0:6379->6379/tcp    52.28.227.0/drunk_thompson
...
{% endhighlight %}

<h3>Utilisation des contraintes</h3>

<p>Ajoutez un label <strong><em>disk=ssd</em></strong> sur un des noeuds du cluster
en éditant à nouveau le fichier <strong><em>/etc/default/docker</em></strong>.</p>

{% highlight bash %}
DOCKER_OPTS="-H tcp://0.0.0.0:2375 --label disk=ssd"
{% endhighlight %}

<p>Redémarrez le démon Docker</p>

{% highlight bash %}
$ sudo systemctl status docker
{% endhighlight %}

<p>Récupérez l'identifiant du conteneur de l'agent Swarm qui s'est arrêté lorsque vous avez
redémarré le démon Docker en utilisant la commande ps -a.</p>

{% highlight bash %}
$ docker ps -a
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS                         NAMES
14fe82905d38        swarm               "/swarm join --advert"   About an hour ago   Up About an hour            2375/tcp                      52.29.87.59/cocky_wright
eb9d893a6df5        swarm               "/swarm join --advert"   About an hour ago   Exited (2) 48 seconds ago                                 52.28.227.0/awesome_noether
{% endhighlight %}

<p>Puis, redémarrez l'agent Swarm en faisant un docker start.</p>

{% highlight bash %}
$ docker start eb9d893a6df5
{% endhighlight %}

<p>Vérifier que le cluster est correctement lancé avec la commande docker info.</p>

{% highlight bash %}
$ docker info
Containers: 10
 Running: 6
 Paused: 0
 Stopped: 4
Images: 14
Server Version: swarm/1.1.3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 2
 52.28.227.0: 52.28.227.0:2375
  └ Status: Healthy
  └ Containers: 5
  └ Reserved CPUs: 2 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: disk=ssd, executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T16:21:04Z
 52.29.87.59: 52.29.87.59:2375
  └ Status: Healthy
  └ Containers: 5
  └ Reserved CPUs: 2 / 2
  └ Reserved Memory: 0 B / 7.669 GiB
  └ Labels: executiondriver=native-0.2, kernelversion=4.2.0-19-generic, operatingsystem=Ubuntu 15.10, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-03-06T16:21:08Z
Plugins: 
 Volume: 
 Network: 
Kernel Version: 4.2.0-19-generic
Operating System: linux
Architecture: amd64
CPUs: 4
Total Memory: 15.34 GiB
Name: dad66ef64043
{% endhighlight %}

<p>Vérifiez que le nouveau label est bien apparu sur le noeud correspondant.</p>

<p>Lancer plusieurs conteneurs avec la contrainte <strong><em>disk=ssd</em></strong> en 
indiquant le label avec la variable d'environnement <strong><em>constraint</em></strong> :</p>

{% highlight bash %}
$ docker run -d -e constraint:disk==ssd redis
{% endhighlight %}

<p>Vérifiez que les conteneurs se sont bien lancés sur le noeud ayant la contrainte.</p>

{% highlight bash %}
$ docker ps
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                         NAMES
3a283675ec09        redis               "/entrypoint.sh redis"   2 seconds ago       Up 1 seconds        6379/tcp                      52.28.227.0/goofy_liskov
70ba2b2b24d6        redis               "/entrypoint.sh redis"   3 seconds ago       Up 2 seconds        6379/tcp                      52.28.227.0/admiring_feynman
d8bb62cf1485        redis               "/entrypoint.sh redis"   4 seconds ago       Up 3 seconds        6379/tcp                      52.28.227.0/sick_mcnulty
{% endhighlight %}

